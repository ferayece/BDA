"""BDA502 REG. SPRING 2017-18

- Gaussian Naive Bayes
- Support Vector Machines
"""

""" Simple Example for GaussianNB from scikit-learn"""

import numpy as np
X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])
Y = np.array([1, 1, 1, 1, 2, 2])
from sklearn.naive_bayes import GaussianNB
clf = GaussianNB(priors=None)  # Modei oluştur, prior yok; bildiğimiz bir yüzde varsa kadın/erkek gibi yazabiliriz.
clf.fit(X, Y)  ##X ve Y arasında bir ilişki kur! MODELI GELISTIRDIGIM YER BURA!
# GaussianNB(priors=None)
print(clf.predict([[-0.8, -1]]))  ## X i böyle verirsem Y ne olur? Tahmin et.

print(clf.class_prior_)  # probability of each class.
print(clf.class_count_)  # number of training samples observed in each class.
print(clf.theta_)  # mean of each feature per class.
print(clf.sigma_)  # variance of each feature per class
print(clf.predict_proba([[0.8, 1]]))  # Return probability estimates for the test vector X.
print(clf.predict_log_proba([[0.8, 1]]))  # Return log-probability estimates for the test vector X.
print(clf.score([[0.8, 1]],[1]))  # Returns the mean accuracy on the given test data and labels.
print(clf.score([[0.8, 1]],[2]))  # Returns the mean accuracy on the given test data and labels.

"""
========================================================================================================================
======================= Classification applications on the handwritten digits data =====================================
========================================================================================================================
In this example, you will see two different applications of Naive Bayesian Algorithm on the digits data set.
"""
print(__doc__)
import pylab as pl
from sklearn.datasets import load_digits
from sklearn.datasets import load_breast_cancer
from sklearn.decomposition import PCA
from time import time
import numpy as np
from sklearn import metrics
from sklearn.cluster import KMeans
from sklearn.preprocessing import scale
from sklearn.model_selection import train_test_split  # some documents still include the cross-validation option but it no more exists in version 18.0
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import confusion_matrix
from sklearn.metrics import accuracy_score
import pylab as plt

########################################################################################################################
##################################### PART A ###########################################################################
########################################################################################################################
np.random.seed(42)  # random seeding is performed
digits = load_digits()  # the whole data set with the labels and other information are extracted
data = scale(digits.data)  # the data is scaled with the use of z-score
print(data)
print(digits)
n_samples, n_features = data.shape  # the no. of samples and no. of features are determined with the help of shape
n_digits = len(np.unique(digits.target))  # the number of labels are determined with the aid of unique formula
labels = digits.target  # get the ground-truth labels into the labels

print(labels)  # the labels are printed on the screen
print(digits.keys())  # this command will provide you the key elements in this dataset
print(digits.data)
print(digits.target)
print(digits.target_names)
print(digits.images)
print(digits.DESCR)  # to get the descriptive information about this dataset

pl.gray()  # turns an image into gray scale
pl.matshow(digits.images[0])
pl.show()
print(digits.images[0])

# Train-test split is one of the most critical issues in ML
# Try this example with different test size like 0.1 or 0.5
y = digits.target
X = digits.data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=10) #X_train VE x_test %70 i ; y_train y_test %30 u. BUNU KESIN BOYLE YAP, RANDOM OLMALI ELLE BÖLME!
print(len(X))
print(len(X_train))

gnb = GaussianNB(priors=None)
# gnb = GaussianNB(priors=[0.1, 0.5, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05]) # 0 ın değerinin dataseti içinde yüzde kaç olduğunu verirsin normalde prior a.
                                                                                      #ama burada 0=0.1 , 1=0.5 vs diye elle ekledik.
fit = gnb.fit(X_train, y_train)
predicted = fit.predict(X_test)
print(confusion_matrix(y_test, predicted)) ##Matrixe bak; 0 ken modelin 0 dediği 49 tane.
print(accuracy_score(y_test, predicted)) # the use of another function for calculating the accuracy (correct_predictions / all_predictions)
print(accuracy_score(y_test, predicted, normalize=False))  # the number of correct predictions,normalize=FALSE toplam tahmine bölmeden yapıyor.
print(len(predicted))  # the number of all of the predictions
unique_p, counts_p = np.unique(predicted, return_counts=True)
print(unique_p, counts_p)
print((predicted == 0).sum())
print(fit.get_params())
print(fit.predict_proba(X))

########################################################################################################################
########################################################################################################################

import numpy as np
X = np.array([[-1, -1], [-2, -1], [1, 1], [2, 1]]) #4item , 2 feature
y = np.array([1, 1, 2, 2])
from sklearn.svm import SVC
clf = SVC() #crete model
clf.fit(X, y)  #initial the model
SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0, degree=3, gamma='auto', kernel='linear',
    max_iter=-1, probability=False, random_state=None, shrinking=True, tol=0.001, verbose=False)
#modeli baştan tanımladık daha çalıştırmadık normalde clf=SVC(...) yapmamız lazım.
#kernel=linear doğrusal olmasını beklerim. Pol dışında bişi seçtiğimde degree önemsiz.
#c ve gamma önemli değerler. c = ceza fonksiyonu, classification hatalı olursa daha yüksek ceza verir c yüksek oldukca. c yükseldikce polynomal hala gelir.
# c çok yüksek olursa overfit etme durumu olur. c düşük olursa underfit olur. En iyisi farklı değerler için çıktıları inceleyip hangisi en iyi sonucu veriyorsa ona karar kılıcaz.
# (c=0, ceza fonk yok,genelde 0.5-10 aralığında kullanılıyor.
print(clf.predict([[-0.8, -1]])) #[1]
print(clf.decision_function([[-0.8, -1]])) #[-0.90840541] bu 2 değeri modele soktuğumuzda; yani (-0.8,-1) noktasının çizilen kernel e mesafesi.
print(clf.get_params())
print(clf.score([[-0.8, -1]],[1])) # 2 feature um olduğu için bii 1 biri 0 çıkacak.
print(clf.score([[-0.8, -1]],[2]))

########################################################################################################################
########################################################################################################################

from sklearn.svm import LinearSVC
from sklearn.datasets import make_blobs
import mglearn

#import numpy as np
#import pylab as plt  mglearn çalışmazsa bunları aç.

X, y = make_blobs(random_state=42)
linear_svm = LinearSVC().fit(X, y)
mglearn.discrete_scatter(X[:, 0], X[:, 1], y)
plt.xlabel("Feature 0")
plt.ylabel("Feature 1")
plt.legend(["Class 0", "Class 1", "Class 2"])
mglearn.discrete_scatter(X[:, 0], X[:, 1], y)
line = np.linspace(-15, 15)

for coef, intercept, color in zip(linear_svm.coef_, linear_svm.intercept_,  ##support vector'lere göre çizgi çiziyor. En yakın noktaya göre kernel=linear margin çiziyor. gammaburada önmli sanırım?
                                  mglearn.cm3.colors):
    print(coef, intercept, color)
    plt.plot(line, -(line * coef[0] + intercept) / coef[1], c=color)

plt.ylim(-10, 15)
plt.xlim(-10, 8)
plt.xlabel("Feature 0")
plt.ylabel("Feature 1")
plt.legend(['Class 0', 'Class 1', 'Class 2', 'Line class 0', 'Line class 1',
            'Line class 2'], loc=(1.01, 0.3))

########################################################################################################################
########################################################################################################################

X, y = make_blobs(centers=4, random_state=8)
y = y % 2
mglearn.discrete_scatter(X[:, 0], X[:, 1], y)
plt.xlabel("Feature 0")
plt.ylabel("Feature 1")
from sklearn.svm import LinearSVC
linear_svm = LinearSVC().fit(X, y)
mglearn.plots.plot_2d_separator(linear_svm, X)
mglearn.discrete_scatter(X[:, 0], X[:, 1], y)
plt.xlabel("Feature 0")
plt.ylabel("Feature 1")
# add the squared first feature
X_new = np.hstack([X, X[:, 1:] ** 2])

from mpl_toolkits.mplot3d import Axes3D, axes3d
figure = plt.figure()
# visualize in 3D
ax = Axes3D(figure, elev=-152, azim=-26)
# plot first all the points with y==0, then all with y == 1
mask = y == 0
ax.scatter(X_new[mask, 0], X_new[mask, 1], X_new[mask, 2], c='b',
           cmap=mglearn.cm2, s=60, edgecolor='k')
ax.scatter(X_new[~mask, 0], X_new[~mask, 1], X_new[~mask, 2], c='r', marker='^',
           cmap=mglearn.cm2, s=60, edgecolor='k')
ax.set_xlabel("feature0")
ax.set_ylabel("feature1")
ax.set_zlabel("feature1 ** 2")

linear_svm_3d = LinearSVC().fit(X_new, y)
coef, intercept = linear_svm_3d.coef_.ravel(), linear_svm_3d.intercept_

# show linear decision boundary
figure = plt.figure()
ax = Axes3D(figure, elev=-152, azim=-26)
xx = np.linspace(X_new[:, 0].min() - 2, X_new[:, 0].max() + 2, 50)
yy = np.linspace(X_new[:, 1].min() - 2, X_new[:, 1].max() + 2, 50)

XX, YY = np.meshgrid(xx, yy)
ZZ = (coef[0] * XX + coef[1] * YY + intercept) / -coef[2]
ax.plot_surface(XX, YY, ZZ, rstride=8, cstride=8, alpha=0.3)
ax.scatter(X_new[mask, 0], X_new[mask, 1], X_new[mask, 2], c='b',
           cmap=mglearn.cm2, s=60, edgecolor='k')
ax.scatter(X_new[~mask, 0], X_new[~mask, 1], X_new[~mask, 2], c='r', marker='^',
           cmap=mglearn.cm2, s=60, edgecolor='k')

ax.set_xlabel("feature0")
ax.set_ylabel("feature1")
ax.set_zlabel("feature1 ** 2")

########################################################################################################################
########################################################################################################################

import numpy as np
import matplotlib.pyplot as plt
from sklearn import datasets
from sklearn import svm
from sklearn.semi_supervised import label_propagation

rng = np.random.RandomState(0)
iris = datasets.load_iris()
X = iris.data[:, :2]
y = iris.target
# step size in the mesh
h = .02
y_30 = np.copy(y)
y_30[rng.rand(len(y)) < 0.3] = -1
y_50 = np.copy(y)
y_50[rng.rand(len(y)) < 0.5] = -1
# we create an instance of SVM and fit out data. We do not scale our
# data since we want to plot the support vectors
ls30 = (label_propagation.LabelSpreading().fit(X, y_30), y_30)
ls50 = (label_propagation.LabelSpreading().fit(X, y_50), y_50)
ls100 = (label_propagation.LabelSpreading().fit(X, y), y)
linear_svc_model = (svm.SVC(kernel='linear').fit(X, y), y)

# create a mesh to plot in
x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
xx, yy = np.meshgrid(np.arange(x_min, x_max, h),
                     np.arange(y_min, y_max, h))

# title for the plots
titles = ['Label Spreading 30% data',
          'Label Spreading 50% data',
          'Label Spreading 100% data',
          'SVC with linear kernel']

color_map = {-1: (1, 1, 1), 0: (0, 0, .9), 1: (1, 0, 0), 2: (.8, .6, 0)}

for i, (clf, y_train) in enumerate((ls30, ls50, ls100, linear_svc_model)):
    # Plot the decision boundary. For that, we will assign a color to each
    # point in the mesh [x_min, x_max]x[y_min, y_max].
    plt.subplot(2, 2, i + 1)
    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])
    # Put the result into a color plot
    Z = Z.reshape(xx.shape)
    plt.contourf(xx, yy, Z, cmap=plt.cm.Paired)
    plt.axis('off')
    # Plot also the training points
    colors = [color_map[y] for y in y_train]
    plt.scatter(X[:, 0], X[:, 1], c=colors, edgecolors='black')
    plt.title(titles[i])

plt.suptitle("Unlabeled points are colored white", y=0.1)
plt.show()


#Ödev: 4 çıktıyı görüyoruz. en alt sağdakini (SVC yi) farklı 4 c ve gamma  değerleri için bakıcaz ve prediction seviyemizi karşılaştırcaz. Accuracy yi karşılaştırcaz.
########################################################################################################################
########################################################################################################################

X, y = make_blobs(centers=4, random_state=8)
y = y % 2

mglearn.discrete_scatter(X[:, 0], X[:, 1], y)
plt.xlabel("Feature 0")
plt.ylabel("Feature 1")

from sklearn.svm import LinearSVC
linear_svm = LinearSVC().fit(X, y)

mglearn.plots.plot_2d_separator(linear_svm, X)
mglearn.discrete_scatter(X[:, 0], X[:, 1], y)
plt.xlabel("Feature 0")
plt.ylabel("Feature 1")

# add the squared first feature
X_new = np.hstack([X, X[:, 1:] ** 2])

from mpl_toolkits.mplot3d import Axes3D, axes3d
figure = plt.figure()
# visualize in 3D
ax = Axes3D(figure, elev=-152, azim=-26)
# plot first all the points with y==0, then all with y == 1
mask = y == 0
ax.scatter(X_new[mask, 0], X_new[mask, 1], X_new[mask, 2], c='b',
           cmap=mglearn.cm2, s=60, edgecolor='k')
ax.scatter(X_new[~mask, 0], X_new[~mask, 1], X_new[~mask, 2], c='r', marker='^',
           cmap=mglearn.cm2, s=60, edgecolor='k')
ax.set_xlabel("feature0")
ax.set_ylabel("feature1")
ax.set_zlabel("feature1 ** 2")

linear_svm_3d = LinearSVC().fit(X_new, y)
coef, intercept = linear_svm_3d.coef_.ravel(), linear_svm_3d.intercept_

# show linear decision boundary
figure = plt.figure()
ax = Axes3D(figure, elev=-152, azim=-26)
xx = np.linspace(X_new[:, 0].min() - 2, X_new[:, 0].max() + 2, 50)
yy = np.linspace(X_new[:, 1].min() - 2, X_new[:, 1].max() + 2, 50)

XX, YY = np.meshgrid(xx, yy)
ZZ = (coef[0] * XX + coef[1] * YY + intercept) / -coef[2]
ax.plot_surface(XX, YY, ZZ, rstride=8, cstride=8, alpha=0.3)
ax.scatter(X_new[mask, 0], X_new[mask, 1], X_new[mask, 2], c='b',
           cmap=mglearn.cm2, s=60, edgecolor='k')
ax.scatter(X_new[~mask, 0], X_new[~mask, 1], X_new[~mask, 2], c='r', marker='^',
           cmap=mglearn.cm2, s=60, edgecolor='k')

ax.set_xlabel("feature0")
ax.set_ylabel("feature1")
ax.set_zlabel("feature1 ** 2")

ZZ = YY ** 2
dec = linear_svm_3d.decision_function(np.c_[XX.ravel(), YY.ravel(), ZZ.ravel()])
plt.contourf(XX, YY, dec.reshape(XX.shape), levels=[dec.min(), 0, dec.max()],
             cmap=mglearn.cm2, alpha=0.5)
mglearn.discrete_scatter(X[:, 0], X[:, 1], y)
plt.xlabel("Feature 0")
plt.ylabel("Feature 1")

########################################################################################################################
########################################################################################################################

from sklearn.svm import SVC
X, y = mglearn.tools.make_handcrafted_dataset()
svm = SVC(kernel='linear', C=10, gamma=0.1).fit(X, y)
# mglearn.plots.plot_2d_separator(svm, X, eps=.5)
# mglearn.discrete_scatter(X[:, 0], X[:, 1], y)

# plot support vectors
sv = svm.support_vectors_
# class labels of support vectors are given by the sign of the dual coefficients
sv_labels = svm.dual_coef_.ravel() > 0
mglearn.discrete_scatter(sv[:, 0], sv[:, 1], sv_labels, s=15, markeredgewidth=3)
plt.xlabel("Feature 0")
plt.ylabel("Feature 1")

fig, axes = plt.subplots(3, 3, figsize=(15, 10))

for ax, C in zip(axes, [-1, 0, 3]):
    for a, gamma in zip(ax, range(-1, 2)):
        mglearn.plots.plot_svm(log_C=C, log_gamma=gamma, ax=a)

axes[0, 0].legend(["class 0", "class 1", "sv class 0", "sv class 1"],
                  ncol=4, loc=(.9, 1.2))



########################################################################################################################
########################################################################################################################

cancer = load_breast_cancer()

X_train, X_test, y_train, y_test = train_test_split(
    cancer.data, cancer.target, random_state=0)

svc = SVC()
svc.fit(X_train, y_train)

print("Accuracy on training set: {:.2f}".format(svc.score(X_train, y_train)))
print("Accuracy on test set: {:.2f}".format(svc.score(X_test, y_test)))

plt.boxplot(X_train, manage_xticks=False)
plt.yscale("symlog")
plt.xlabel("Feature index")
plt.ylabel("Feature magnitude")

# Compute the minimum value per feature on the training set
min_on_training = X_train.min(axis=0)
# Compute the range of each feature (max - min) on the training set
range_on_training = (X_train - min_on_training).max(axis=0)

# subtract the min, divide by range
# afterward, min=0 and max=1 for each feature
X_train_scaled = (X_train - min_on_training) / range_on_training
print("Minimum for each feature\n{}".format(X_train_scaled.min(axis=0)))
print("Maximum for each feature\n {}".format(X_train_scaled.max(axis=0)))

# use THE SAME transformation on the test set,
# using min and range of the training set. See Chapter 3 (unsupervised learning) for details.
X_test_scaled = (X_test - min_on_training) / range_on_training

svc = SVC(C=1.0, kernel="linear", degree=3, gamma="auto", coef0=0.0, shrinking=True, probability=False, tol=0.001,
        cache_size=200, class_weight=None, verbose=False, max_iter=-1, decision_function_shape="ovr", random_state=None)
svc.fit(X_train_scaled, y_train)
print("Accuracy on training set: {:.3f}".format(svc.score(X_train_scaled, y_train)))
print("Accuracy on test set: {:.3f}".format(svc.score(X_test_scaled, y_test)))


from sklearn.svm import SVC
X, y = mglearn.tools.make_handcrafted_dataset()
svm = SVC(kernel='rbf', C=10, gamma=0.1).fit(X, y)
mglearn.plots.plot_2d_separator(svm, X, eps=.5)
mglearn.discrete_scatter(X[:, 0], X[:, 1], y)
# plot support vectors
sv = svm.support_vectors_
print(sv)
# class labels of support vectors are given by the sign of the dual coefficients
sv_labels = svm.dual_coef_.ravel() > 0
print(sv_labels)
mglearn.discrete_scatter(sv[:, 0], sv[:, 1], sv_labels, s=15, markeredgewidth=3)
plt.xlabel("Feature 0")
plt.ylabel("Feature 1")

fig, axes = plt.subplots(3, 3, figsize=(15, 10))
for ax, C in zip(axes, [-1, 0, 3]):
    for a, gamma in zip(ax, range(-1, 2)):
        mglearn.plots.plot_svm(log_C=C, log_gamma=gamma, ax=a)

axes[0, 0].legend(["class 0", "class 1", "sv class 0", "sv class 1"],
                  ncol=4, loc=(.9, 1.2))


########################################################################################################################
########################################################################################################################



cancer = load_breast_cancer()
X_train, X_test, y_train, y_test = train_test_split(cancer.data, cancer.target, stratify=cancer.target, random_state=42)
# ...
# ...
# ...
print("Accuracy on training set: {:.3f}".format(model_A.score(X_train, y_train)))
print("Accuracy on test set: {:.3f}".format(model_A.score(X_test, y_test)))

def plot_feature_importances_cancer(model):
    n_features = cancer.data.shape[1]
    plt.barh(range(n_features), model.feature_importances_, align='center')
    plt.yticks(np.arange(n_features), cancer.feature_names)
    plt.xlabel("Feature importance")
    plt.ylabel("Feature")
    plt.ylim(-1, n_features)

plot_feature_importances_cancer(model_A)
