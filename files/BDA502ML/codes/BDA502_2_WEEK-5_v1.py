"""
===========================================================
BDA502 - WEEK#5, Spring 2017-2018
===========================================================
"""

########################################################################################################################
########################################################################################################################


from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split
cancer = load_breast_cancer()

X_train, X_test, y_train, y_test = train_test_split(cancer.data, cancer.target,
                                                    random_state=1)
print(X_train.shape)
print(X_test.shape)

from sklearn.preprocessing import MinMaxScaler
scaler = MinMaxScaler()
#print(scaler)
scaler.fit(X_train)  # modeli X_train'e göre geliştirdik.
# transform data
print(X_train)
X_train_scaled = scaler.transform(X_train)   #X_train datasetini 0-1 aralığına çekti.
print(X_train_scaled) ## 0-1 aralığına çektik. (feature scaling; range ler farklı olursa feature'ların birbirine baskın olma durumu oluyor. bu sebeple yapıyoruz)
# print dataset properties before and after scaling
print("transformed shape: {}".format(X_train_scaled.shape))
print("per-feature minimum before scaling:\n {}".format(X_train.min(axis=0))) # her feature'un minimumunu döndürür.
print("per-feature maximum before scaling:\n {}".format(X_train.max(axis=0)))   # her feature'un maximumunu döndürür.
print("per-feature minimum after scaling:\n {}".format(X_train_scaled.min(axis=0))) #of course 0.
print("per-feature maximum after scaling:\n {}".format(X_train_scaled.max(axis=0)))  #of course 1.
# transform test data
X_test_scaled = scaler.transform(X_test)  #scaler ı yukarıda fit ettik Train datasına re. dolayısı ile train'in max-min'ini biliyor. X_testi x_train'e göre scale edecek.
# print test data properties after scaling
print("per-feature minimum after scaling:\n{}".format(X_test_scaled.min(axis=0)))
print("per-feature maximum after scaling:\n{}".format(X_test_scaled.max(axis=0)))
# print test data properties after scaling;
#min-max da amaç 0-1 arasına çıkmak. Train'de transform ettik, gördük; x_trainde olmayan daha büyük bir değer varsa x_test'de
#0-1 arasında değerler göremeyiz. Sıkıntı.
#minMax = (x-min) / (max-min)  --> buradaki max-min değeri train'e göre.

########################################################################################################################
########################################################################################################################

import mglearn
from sklearn.datasets import make_blobs
import pylab as plt
# make synthetic data
X, _ = make_blobs(n_samples=50, centers=5, random_state=4, cluster_std=2)
# split it into training and test sets
X_train, X_test = train_test_split(X, random_state=5, test_size=.1)
# plot the training and test sets
fig, axes = plt.subplots(1, 3, figsize=(13, 4))
axes[0].scatter(X_train[:, 0], X_train[:, 1], c=mglearn.cm2(0), label="Training set", s=60)
axes[0].scatter(X_test[:, 0], X_test[:, 1], marker='^', c=mglearn.cm2(1), label="Test set", s=60)
axes[0].legend(loc='upper left')
axes[0].set_title("Original Data")
# scale the data using MinMaxScaler
scaler = MinMaxScaler()
scaler.fit(X_train)
X_train_scaled = scaler.transform(X_train)
X_test_scaled = scaler.transform(X_test)
# visualize the properly scaled data
axes[1].scatter(X_train_scaled[:, 0], X_train_scaled[:, 1], c=mglearn.cm2(0), label="Training set", s=60)
axes[1].scatter(X_test_scaled[:, 0], X_test_scaled[:, 1], marker='^', c = mglearn.cm2(1), label="Test set", s=60)
axes[1].set_title("Scaled Data")

# rescale the test set separately
# so test set min is 0 and test set max is 1
# DO NOT DO THIS! For illustration purposes only.
test_scaler = MinMaxScaler()
test_scaler.fit(X_test)  #teste göre tekrar fit etmiş. Train eksenini bozduk.Improper oldu bu yüzden.
X_test_scaled_badly = test_scaler.transform(X_test)

# visualize wrongly scaled data
axes[2].scatter(X_train_scaled[:, 0], X_train_scaled[:, 1],
    c = mglearn.cm2(0), label="training set", s=60)
axes[2].scatter(X_test_scaled_badly[:, 0], X_test_scaled_badly[:, 1],
    marker = '^', c=mglearn.cm2(1), label="test set", s=60)
axes[2].set_title("Improperly Scaled Data")

for ax in axes:
    ax.set_xlabel("Feature 0")
    ax.set_ylabel("Feature 1")
fig.tight_layout()


from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
# calling fit and transform in sequence (using method chaining)
X_scaled = scaler.fit(X).transform(X)
# same result, but more efficient computation
X_scaled_d = scaler.fit_transform(X)

from sklearn.svm import SVC  ##normal data
X_train, X_test, y_train, y_test = train_test_split(cancer.data, cancer.target, random_state=0)
svm = SVC(C=100)
svm.fit(X_train, y_train)
print("Test set accuracy: {:.2f}".format(svm.score(X_test, y_test)))
print("Test set accuracy: {:.2f}".format(svm.score(X_test, y_test)))

# preprocessing using 0-1 scaling
scaler = MinMaxScaler()
scaler.fit(X_train)
X_train_scaled = scaler.transform(X_train)
X_test_scaled = scaler.transform(X_test)
# learning an SVM on the scaled training data
svm.fit(X_train_scaled, y_train)
# scoring on the scaled test set
print("Scaled test set accuracy: {:.2f}".format(svm.score(X_test_scaled, y_test)))
print("Scaled test set accuracy: {:.2f}".format(svm.score(X_train_scaled, y_train))) #compare for overfitting. (0.97 -0.99 çok yakın;overfit değil.)


# preprocessing using zero mean and unit variance scaling
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
scaler.fit(X_train)
X_train_scaled = scaler.transform(X_train)
X_test_scaled = scaler.transform(X_test)

# learning an SVM on the scaled training data
svm.fit(X_train_scaled, y_train)
# scoring on the scaled test set
print("SVM test accuracy: {:.2f}".format(svm.score(X_test_scaled, y_test)))
print("SVM test accuracy: {:.2f}".format(svm.score(X_train_scaled, y_train)))
# TASK-1: Please provide different scaling methods for the same data set ana compare them.
#  Scaled_data's   (scaled with MinMaxScaler()) accuracy is much more better than orginial data. (Overfit mi? Train test arasındaki farka bakalım. Değil.)
#  Scaled data's (scaled with StandardScaler())accuracy  is 0.96 so for this dataset MinMaxScaler() is better.
# Without scale; there is overfitting. Train score=0.99, test score=0.63.(fark=0.36 OVERFIT!) With scale; train score=0.99, test score=0.97 So; when we scale data,
# there is no overfitting.(scale ile train-test arasındaki fark çok az.(0.02 fark))
#######################################################################################################################
#######################################################################################################################

"""
===========================================================
A demo of K-Means clustering on the iris dataset
===========================================================
In this example we compare the various initialization strategies for K-means in terms of runtime and quality of the results.

As the ground truth is known here, we also apply different cluster quality metrics to judge the goodness of fit of the cluster
labels to the ground truth.

http://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html

"""
#KMeans; clustering (grouping data according to similarity)diince aklımıza ilk gelendir.
#Adv: Basit ve hızlı.
#Yeni gelen noktanın;Küme içinde olan noktalara uzaklığı ve diğer kümelerin içindkei noktaya uzaklığı önemli.
# Iteration sayısı çok önemli. Iteratıon sayısının ne çok düşük(1) ve ne çok yüksek(PC yi yormadan) bi sayı vermeli.
#Kaça böleceğimizi nasıl biliriz?  (elbow point i bul.)
#Küme sayısını bilmiyoruz; küme sayısını biz veriyoruz. (Disadv:deneme yanılma yöntemine dayalı)
# Kümeleme yaptım; score'a bakıyorum.
#label ımız varken yapabiliyoruz; yeni gelen datayı bildiğimze göre labellıyoruz.
#Ground root ne bilmediğim durumlar: müşteri segmentation da davranıssal pattern'den yola çıkarak bakıyorsam;
#kümelere birbirine yakın ise bir şüphe: acaba yanlış mı kümeledik?Nekadar uzaksa o kadar iyi.

from mpl_toolkits.mplot3d import Axes3D  # to have 3d figures
from sklearn import datasets  # to obtain the dataset of iris
from time import time  # to keep track of the processing time
import numpy as np  # numpy library as you already know
import matplotlib.pyplot as plt
from sklearn import metrics
from sklearn.cluster import KMeans  # to use the kmeans clustering algorithm
from sklearn.preprocessing import scale  # to be able to preprocess by scaling the data before clustering: See explanations in A2 for this

np.random.seed(5)
centers = [[1, 1], [-1, -1], [1, -1]] #normalde rando molara kda atabiliriz;
#random yerine K-MEANS++ ile daha uygun olabilcek noktaları seçip daha hızl ışekilde merkezleri set edebiliriz.
iris = datasets.load_iris()  # extract the dataset into iris
X = iris.data  # the data part will be here
y = iris.target  # the target items (class labels) will be here
data = scale(iris.data)  # use of scale function here to standardize a dataset along any axis. Center to the mean and component wise scale to unit variance
print(iris.data)  # to see the difference between the original data and scaled data lets print them
print(data)  # this is the scaled one
n_samples, n_features = data.shape  # n_samples will contain the number of samples/cases we have for the dataset we have. Here it is 150
# n_features will contain the number of features/dimensions, here, it is 4 as we mentioned: sepal_length, sepal_width, petal_length, and petal_width
no_spec = len(np.unique(iris.target)) # number of species, here we have three of them; iris dataset  te 3 unique label vardı.
labels = iris.target  # name of the classes we have (three different species)
sample_size = 150
# Below, we provide 3 different analyses with different setting as you can see below:
estimators = {'k_means_iris_3': KMeans(n_clusters=3),  # run k-means with 3 clusters,dictionary #default merkez bulma yöntemi:K-Means++ ; random yapabilirsin.
              'k_means_iris_8': KMeans(n_clusters=8),
              'k_means_iris_2': KMeans(n_clusters=2)}  # run k-means with 8 clusters,dictionary

# Task-2: Please prepare this code to run for k= 2, 3, 4, 5, 6, 7, 8
#estimators includes number of cluster.We can add these given number into estimators dictionary to use as cluster.
########################################################################################################################
# This section below is for illustrating the dataset in 3-D regarding different features
########################################################################################################################

fignum = 1
for name, est in estimators.items():  # cycle regarding the number of analysis determined above
    fig_n_clusters = est.n_clusters  # number of clusters desired in the kmeans analyses
    fig = plt.figure(fignum, figsize=(4, 3))  # figure descriptions
    plt.clf()  # clear the figure
    ax = Axes3D(fig, rect=[0, 0, .95, 1], elev=48, azim=134)  # module containing Axes3D, an object which can plot 3D objects on a 2D matplotlib figure
    plt.cla()  # clear the axis
    est.fit(X)  # fit function compute kmeans clusterimg given the array-like input, X here: training instances to cluster
    labels = est.labels_  # estimated labels
    ax.scatter(X[:, 3], X[:, 0], X[:, 2], c=labels.astype(np.float))   # to have a 3D scatter plot we are using the 3 dimensions you can also try 1
    ax.w_xaxis.set_ticklabels([])  # Set the text values of the tick labels. Return a list of Text instances.
    ax.w_yaxis.set_ticklabels([])
    ax.w_zaxis.set_ticklabels([])
    ax.set_xlabel('Petal width')  # label the related axis
    ax.set_ylabel('Sepal length')
    ax.set_zlabel('Petal length')
    fignum = fignum + 1
    plt.suptitle(("Cluster count = %d" % fig_n_clusters), fontsize=14, fontweight='bold')  #provide a title for the figure

fig = plt.figure(fignum, figsize=(4, 3))
plt.clf()
ax = Axes3D(fig, rect=[0, 0, .95, 1], elev=48, azim=134)
plt.cla()
for name, label in [('Setosa', 0), ('Versicolour', 1), ('Virginica', 2)]:
    ax.text3D(X[y == label, 3].mean(), X[y == label, 0].mean() + 1.5, X[y == label, 2].mean(), name, horizontalalignment='center', bbox=dict(alpha=.5, edgecolor='w', facecolor='w'))
# Reorder the labels to have colors matching the cluster results
y = np.choose(y, [1, 2, 0]).astype(np.float)
ax.scatter(X[:, 3], X[:, 0], X[:, 2], c=y)
ax.w_xaxis.set_ticklabels([])
ax.w_yaxis.set_ticklabels([])
ax.w_zaxis.set_ticklabels([])
ax.set_xlabel('Petal width')
ax.set_ylabel('Sepal length')
ax.set_zlabel('Petal length')
plt.suptitle(("Cluster number = %d" % no_spec), fontsize=14, fontweight='bold')
plt.show()



# TASK-3: Please draw the figure for 2-means (k = 2).

########################################################################################################################
########################################################################################################################
#Küme sayısını nasıl belirleyeceğiz? Grafiğe bkaıp bu böyle diyemeyiz. Sadece bir kümeleme oldu diyebildik yukarıdaki kodla.
#1. K-Meanstaki en büyük sıkıntı: outliers. Onları elemeliyiz.
#2. Scale etmeliyim.
#3. Kümelemeyi lokal merkeze göre yapıp günü kurtaracak yapabilir.
print(82 * '_')
print('init\t\ttime\tinertia\thomo\tcompl\tv-meas\tARI\t\tAMI\t\tsilhouette')

def bench_k_means(estimator, name, data):  # define a function to run kmeans with different settings easily
    t0 = time()   # getting the starting time of the process
    estimator.fit(data)  # This line will provide the preparation of the settings as such: KMeans(copy_x=True, init='random', max_iter=300, n_clusters=3, n_init=10, n_jobs=1, precompute_distances='auto', random_state=None, tol=0.0001, verbose=0)
    print('%-9s\t%.2fs\t%i\t\t%.3f\t%.3f\t%.3f\t%.3f\t%.3f\t%.3f'
          % (name, (time() - t0), estimator.inertia_,
             metrics.homogeneity_score(labels, estimator.labels_), #herhangi bir cluster oluşturduğumuzda; tüm noktaların bu sınıf tarafından kapsanması. Tüm noktalar kapsanmışssa=1 olur. Homojenlikte hepsi tek bir class'a aitse homojen denilir.
             metrics.completeness_score(labels, estimator.labels_), #all memers of the same class(gerçek kümesi,label'ı) are in the same cluster(üretilen küme); homo ile fark
             metrics.v_measure_score(labels, estimator.labels_), #homo ve completeness'dan hesaplanır.(harmonik ortalama) cluster labeling konusunda iyi bi measure.
             metrics.adjusted_rand_score(labels, estimator.labels_),
             metrics.adjusted_mutual_info_score(labels, estimator.labels_),#2'den fazla küme.
             metrics.silhouette_score(data, estimator.labels_, metric='euclidean', sample_size=n_samples)))  #önemli.Labelsız data da da kullanılır.
    #en kritiği:silüet skor.(1/-1 arasında)   1: süper bir kümeleme, <0.5 kötü kümeleme, <0.2 kaale alma bok gibi kümeleme.
    #silüet skor: kümeleme ne kadar başarılı?Yeni küme oluşturdukça düşme eğiliminde. Burda elbow kuralına bakmalıyız. Düşme eğilimindeyken aynı seviyede kaldı
    #ya da çok az düştüyse silüet skoru; o nokta elbow pointtir. Durmam ereken küme sayısı odur.

# Let us run it with the original data no_spec = 3 as initially set.
#algoritma farkı:
bench_k_means(KMeans(init='k-means++', n_clusters=no_spec, n_init=10), name="k-means++", data=iris.data) #no_spec=3
bench_k_means(KMeans(init='random', n_clusters=no_spec, n_init=10), name="random", data=iris.data) #no_spec=3
#
bench_k_means(KMeans(init='k-means++', n_clusters=8, n_init=10), name="k-means++", data=iris.data)  #daha yavaş çalıştı; prprocess var çünkü k-means'te. Random da yok.
bench_k_means(KMeans(init='random', n_clusters=8, n_init=10), name="random", data=iris.data)
# Let us try it with the scaled data (the settings are the same)

print('init\t\ttime\tinertia\thomo\tcompl\tv-meas\tARI\t\tAMI\t\tsilhouette')
print(82 * '_')
#bench_k_means(KMeans(init='k-means++', n_clusters=2, n_init=10), name="k-means++", data=iris.data) # scaled data
#bench_k_means(KMeans(init='random', n_clusters=2, n_init=10), name="random", data=iris.data)
bench_k_means(KMeans(init='k-means++', n_clusters=no_spec, n_init=10), name="k-means++", data=iris.data) # scaled data
bench_k_means(KMeans(init='random', n_clusters=no_spec, n_init=10), name="random", data=iris.data)
bench_k_means(KMeans(init='k-means++', n_clusters=4, n_init=10), name="k-means++", data=iris.data)
bench_k_means(KMeans(init='random', n_clusters=4, n_init=10), name="random", data=iris.data)
bench_k_means(KMeans(init='k-means++', n_clusters=5, n_init=10), name="k-means++", data=iris.data)
bench_k_means(KMeans(init='random', n_clusters=5, n_init=10), name="random", data=iris.data)
bench_k_means(KMeans(init='k-means++', n_clusters=8, n_init=10), name="k-means++", data=iris.data)
bench_k_means(KMeans(init='random', n_clusters=8, n_init=10), name="random", data=iris.data)
print(82 * '_')

#find centers:

kmeans = KMeans(init='k-means++' ,n_clusters=no_spec, n_init=10).fit(iris.data)
kmeans.predict([[0, 0,1,1], [4, 4,2,2]])
#print(kmeans.labels_)
print(kmeans.cluster_centers_)

mm= np.matrix(kmeans.cluster_centers_)

distinca= sqrt()
print(mm)
#neden scaled data'da silüet düştü? Z-Score'a göre hesaplıyor; veri normal bir dağılım göstermediği için. (ödevdeki gibi)
#cluster aynıyken silüet score. (scaled data mı normal data mı?)
# TASK-4: Please do this calculation for k = 2, 3, 4, 5, 6, 7, 8

#######################################################################################################################


from __future__ import print_function

from sklearn.datasets import make_blobs
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_samples, silhouette_score

import matplotlib.pyplot as plt
import matplotlib.cm as cm
import numpy as np

print(__doc__)


from mpl_toolkits.mplot3d import Axes3D  # to have 3d figures
from sklearn import datasets  # to obtain the dataset of iris
from time import time  # to keep track of the processing time
import numpy as np  # numpy library as you already know
import matplotlib.pyplot as plt
from sklearn import metrics
from sklearn.cluster import KMeans  # to use the kmeans clustering algorithm
from sklearn.preprocessing import scale  # to be able to preprocess by scaling the data before clustering: See explanations in A2 for this

########################################################################################################################
########################################################################################################################
#soldaki grafikler:
#her bir nokta için silüet noktasını hesaplıyor. Kümeye göre peşpeşe dizilmiş.
#sağdaki grafikler: birbirinden ne kadar ayrıştırırsak silüet skoru düşüyor.
a = np.random.seed(5)
print(a)
centers = [[1, 1], [-1, -1], [1, -1]]
iris = datasets.load_iris()  # extract the dataset into iris
data = scale(iris.data)  # use of scale function here to standardize a dataset along any axis. Center to the mean and component wise scale to unit variance
X = iris.data  # the data part will be here
y = iris.target  # the target items (class labels) will be here

n_samples, n_features = data.shape  # n_samples will contain the number of samples/cases we have for the dataset we have. Here it is 150
# n_features will contain the number of features/dimensions, here, it is 4 as we mentioned: sepal_length, sepal_width, petal_length, and petal_width
no_spec = len(np.unique(iris.target)) # number of species, here we have three of them
labels = iris.target  # name of the classes we have (three different species)


# Generating the sample data from make_blobs
# This particular setting has one distinct cluster and 3 clusters placed close
# together.
"""X, y = make_blobs(n_samples=500,
                  n_features=2,
                  centers=4,
                  cluster_std=1,
                  center_box=(-10.0, 10.0),
                  shuffle=True,
                  random_state=1)  # For reproducibility

# range_n_clusters = [2, 3, 4, 5, 6, 7, 8]"""

range_n_clusters = [2, 3, 4, 5, 6, 7, 8]

for n_clusters in range_n_clusters:
    # Create a subplot with 1 row and 2 columns
    fig, (ax1, ax2) = plt.subplots(1, 2)
    fig.set_size_inches(18, 7)

    # The 1st subplot is the silhouette plot
    # The silhouette coefficient can range from -1, 1 but in this example all
    # lie within [-0.1, 1]
    ax1.set_xlim([-0.1, 1])
    # The (n_clusters+1)*10 is for inserting blank space between silhouette
    # plots of individual clusters, to demarcate them clearly.
    ax1.set_ylim([0, len(X) + (n_clusters + 1) * 10])

    # Initialize the clusterer with n_clusters value and a random generator
    # seed of 10 for reproducibility.
    clusterer = KMeans(n_clusters=n_clusters, random_state=10)
    cluster_labels = clusterer.fit_predict(X)

    # The silhouette_score gives the average value for all the samples.
    # This gives a perspective into the density and separation of the formed
    # clusters
    silhouette_avg = silhouette_score(X, cluster_labels)
    print("For n_clusters =", n_clusters,
          "The average silhouette_score is :", silhouette_avg)

    # Compute the silhouette scores for each sample
    sample_silhouette_values = silhouette_samples(X, cluster_labels)

    y_lower = 10
    for i in range(n_clusters):
        # Aggregate the silhouette scores for samples belonging to
        # cluster i, and sort them
        ith_cluster_silhouette_values = \
            sample_silhouette_values[cluster_labels == i]

        ith_cluster_silhouette_values.sort()

        size_cluster_i = ith_cluster_silhouette_values.shape[0]
        y_upper = y_lower + size_cluster_i

        color = cm.spectral(float(i) / n_clusters)
        ax1.fill_betweenx(np.arange(y_lower, y_upper),
                          0, ith_cluster_silhouette_values,
                          facecolor=color, edgecolor=color, alpha=0.7)

        # Label the silhouette plots with their cluster numbers at the middle
        ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))

        # Compute the new y_lower for next plot
        y_lower = y_upper + 10  # 10 for the 0 samples

    ax1.set_title("The silhouette plot for the various clusters.")
    ax1.set_xlabel("The silhouette coefficient values")
    ax1.set_ylabel("Cluster label")

    # The vertical line for average silhouette score of all the values
    ax1.axvline(x=silhouette_avg, color="red", linestyle="--")

    ax1.set_yticks([])  # Clear the yaxis labels / ticks
    ax1.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])

    # 2nd Plot showing the actual clusters formed
    colors = cm.spectral(cluster_labels.astype(float) / n_clusters)
    ax2.scatter(X[:, 0], X[:, 1], marker='.', s=30, lw=0, alpha=0.7,
                c=colors)

    # Labeling the clusters
    centers = clusterer.cluster_centers_
    # Draw white circles at cluster centers
    ax2.scatter(centers[:, 0], centers[:, 1],
                marker='o', c="white", alpha=1, s=200)

    for i, c in enumerate(centers):
        ax2.scatter(c[0], c[1], marker='$%d$' % i, alpha=1, s=50)

    ax2.set_title("The visualization of the clustered data.")
    ax2.set_xlabel("Feature space for the 1st feature")
    ax2.set_ylabel("Feature space for the 2nd feature")

    plt.suptitle(("Silhouette analysis for KMeans clustering on sample data "
                  "with n_clusters = %d" % n_clusters),
                 fontsize=14, fontweight='bold')

    plt.show()


# TASK-5: Please provide comments for the given graphs. What do they represent?



########################################################################################################################
########################################################################################################################


# pg. 168
mglearn.plots.plot_kmeans_algorithm()
mglearn.plots.plot_kmeans_boundaries()
from sklearn.datasets import make_blobs
from sklearn.cluster import KMeans

# generate synthetic two-dimensional data
X, y = make_blobs(random_state=1)
# build the clustering model
kmeans = KMeans(n_clusters=3)
kmeans.fit(X)
print("Cluster memberships:\n{}".format(kmeans.labels_))
print(kmeans.predict(X))

mglearn.discrete_scatter(X[:, 0], X[:, 1], kmeans.labels_, markers='o')
mglearn.discrete_scatter(
kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], [0, 1, 2], markers='^', markeredgewidth=2)

fig, axes = plt.subplots(1, 2, figsize=(10, 5))
# using two cluster centers:
kmeans = KMeans(n_clusters=2)
kmeans.fit(X)
assignments = kmeans.labels_
mglearn.discrete_scatter(X[:, 0], X[:, 1], assignments, ax=axes[0])

# using five cluster centers:
kmeans = KMeans(n_clusters=5)
kmeans.fit(X)
assignments = kmeans.labels_
mglearn.discrete_scatter(X[:, 0], X[:, 1], assignments, ax=axes[1])

X_varied, y_varied = make_blobs(n_samples=200,
cluster_std=[1.0, 2.5, 0.5], random_state=170)
y_pred = KMeans(n_clusters=3, random_state=0).fit_predict(X_varied)
mglearn.discrete_scatter(X_varied[:, 0], X_varied[:, 1], y_pred)
plt.legend(["cluster 0", "cluster 1", "cluster 2"], loc='best')
plt.xlabel("Feature 0")
plt.ylabel("Feature 1")

# generate some random cluster data
X, y = make_blobs(random_state=170, n_samples=600)
rng = np.random.RandomState(74)
# transform the data to be stretched
transformation = rng.normal(size=(2, 2))
X = np.dot(X, transformation)

# cluster the data into three clusters
kmeans = KMeans(n_clusters=3)
kmeans.fit(X)
y_pred = kmeans.predict(X)
# plot the cluster assignments and cluster centers
plt.scatter(X[:, 0], X[:, 1], c=y_pred, cmap=mglearn.cm3)
plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], marker='^', c=[0, 1, 2], s=100, linewidth=2, cmap=mglearn.cm3)
plt.xlabel("Feature 0")
plt.ylabel("Feature 1")

########################################################################################################################

# generate synthetic two_moons data (with less noise this time)
"""from sklearn.datasets import make_moons
X, y = make_moons(n_samples=200, noise=0.05, random_state=0)
# cluster the data into two clusters
kmeans = KMeans(n_clusters=2)
kmeans.fit(X)
y_pred = kmeans.predict(X)

# plot the cluster assignments and cluster centers
plt.scatter(X[:, 0], X[:, 1], c=y_pred, cmap=mglearn.cm2, s=60)
plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1],
marker='^', c=[mglearn.cm2(0), mglearn.cm2(1)], s=100, linewidth=2)
plt.xlabel("Feature 0")
plt.ylabel("Feature 1")

X_train, X_test, y_train, y_test = train_test_split(
X_people, y_people, stratify=y_people, random_state=0)
nmf = NMF(n_components=100, random_state=0)
nmf.fit(X_train)
pca = PCA(n_components=100, random_state=0)
pca.fit(X_train)
kmeans = KMeans(n_clusters=100, random_state=0)
kmeans.fit(X_train)
X_reconstructed_pca = pca.inverse_transform(pca.transform(X_test))
X_reconstructed_kmeans = kmeans.cluster_centers_[kmeans.predict(X_test)]
X_reconstructed_nmf = np.dot(nmf.transform(X_test), nmf.components_)
fig, axes = plt.subplots(3, 5, figsize=(8, 8),
subplot_kw={'xticks': (), 'yticks': ()})
fig.suptitle("Extracted Components")
for ax, comp_kmeans, comp_pca, comp_nmf in zip( axes.T, kmeans.cluster_centers_, pca.components_, nmf.components_):
    ax[0].imshow(comp_kmeans.reshape(image_shape))
    ax[1].imshow(comp_pca.reshape(image_shape), cmap='viridis')
    ax[2].imshow(comp_nmf.reshape(image_shape))
    axes[0, 0].set_ylabel("kmeans")
    axes[1, 0].set_ylabel("pca")
    axes[2, 0].set_ylabel("nmf")

fig, axes = plt.subplots(4, 5, subplot_kw={'xticks': (), 'yticks': ()},
figsize=(8, 8))
fig.suptitle("Reconstructions")
for ax, orig, rec_kmeans, rec_pca, rec_nmf in zip(
    axes.T, X_test, X_reconstructed_kmeans, X_reconstructed_pca, X_reconstructed_nmf):

    ax[0].imshow(orig.reshape(image_shape))
    ax[1].imshow(rec_kmeans.reshape(image_shape))
    ax[2].imshow(rec_pca.reshape(image_shape))
    ax[3].imshow(rec_nmf.reshape(image_shape))
    axes[0, 0].set_ylabel("original")
    axes[1, 0].set_ylabel("kmeans")
    axes[2, 0].set_ylabel("pca")
    axes[3, 0].set_ylabel("nmf")

from scipy.cluster.hierarchy import dendrogram
from sklearn.cluster import AgglomerativeClustering
X, y = make_blobs(random_state=1)
agg = AgglomerativeClustering(n_clusters=3)
assignment = agg.fit_predict(X)
mglearn.discrete_scatter(X[:, 0], X[:, 1], assignment)
plt.xlabel("Feature 0")
plt.ylabel("Feature 1")
mglearn.plots.plot_agglomerative_algorithm()
distance_features = kmeans.transform(X)
print("Distance feature shape: {}".format(distance_features.shape))
print("Distance features:\n{}".format(distance_features))

# Now we plot the dendrogram for the linkage_array containing the distances between clusters
dendrogram(linkage_array)

# Mark the cuts in the tree that signify two or three clusters
ax = plt.gca()
bounds = ax.get_xbound()
ax.plot(bounds, [7.25, 7.25], '--', c='k')
ax.plot(bounds, [4, 4], '--', c='k')
ax.text(bounds[1], 7.25, ' two clusters', va='center', fontdict={'size': 15})
ax.text(bounds[1], 4, ' three clusters', va='center', fontdict={'size': 15})
plt.xlabel("Sample index")
plt.ylabel("Cluster distance")


from sklearn.cluster import DBSCAN
X, y = make_blobs(random_state=0, n_samples=12)
dbscan = DBSCAN()
clusters = dbscan.fit_predict(X)
print("Cluster memberships:\n{}".format(clusters))
mglearn.plots.plot_dbscan()
X, y = make_moons(n_samples=200, noise=0.05, random_state=0)
# rescale the data to zero mean and unit variance
scaler = StandardScaler()
scaler.fit(X)
X_scaled = scaler.transform(X)
dbscan = DBSCAN()
clusters = dbscan.fit_predict(X_scaled)
# plot the cluster assignments
plt.scatter(X_scaled[:, 0], X_scaled[:, 1], c=clusters, cmap=mglearn.cm2, s=60)
plt.xlabel("Feature 0")
plt.ylabel("Feature 1")


from sklearn.metrics.cluster import adjusted_rand_score
X, y = make_moons(n_samples=200, noise=0.05, random_state=0)
# rescale the data to zero mean and unit variance
scaler = StandardScaler()
scaler.fit(X)
X_scaled = scaler.transform(X)
fig, axes = plt.subplots(1, 4, figsize=(15, 3),
subplot_kw={'xticks': (), 'yticks': ()})
# make a list of algorithms to use
algorithms = [KMeans(n_clusters=2), AgglomerativeClustering(n_clusters=2),
DBSCAN()]
# create a random cluster assignment for reference
random_state = np.random.RandomState(seed=0)
random_clusters = random_state.randint(low=0, high=2, size=len(X))
# plot random assignment
axes[0].scatter(X_scaled[:, 0], X_scaled[:, 1], c=random_clusters, cmap=mglearn.cm3, s=60)
axes[0].set_title("Random assignment - ARI: {:.2f}".format(
adjusted_rand_score(y, random_clusters)))

for ax, algorithm in zip(axes[1:], algorithms):
# plot the cluster assignments and cluster centers
    clusters = algorithm.fit_predict(X_scaled)
ax.scatter(X_scaled[:, 0], X_scaled[:, 1], c=clusters,  cmap=mglearn.cm3, s=60)
ax.set_title("{} - ARI: {:.2f}".format(algorithm.__class__.__name__,
adjusted_rand_score(y, clusters)))


from sklearn.metrics import accuracy_score
# these two labelings of points correspond to the same clustering
clusters1 = [0, 0, 1, 1, 0]
clusters2 = [1, 1, 0, 0, 1]
# accuracy is zero, as none of the labels are the same
print("Accuracy: {:.2f}".format(accuracy_score(clusters1, clusters2)))
# adjusted rand score is 1, as the clustering is exactly the same
print("ARI: {:.2f}".format(adjusted_rand_score(clusters1, clusters2)))


from sklearn.metrics.cluster import silhouette_score
X, y = make_moons(n_samples=200, noise=0.05, random_state=0)
# rescale the data to zero mean and unit variance
scaler = StandardScaler()
scaler.fit(X)
X_scaled = scaler.transform(X)
fig, axes = plt.subplots(1, 4, figsize=(15, 3),
subplot_kw={'xticks': (), 'yticks': ()})
# create a random cluster assignment for reference
random_state = np.random.RandomState(seed=0)
random_clusters = random_state.randint(low=0, high=2, size=len(X))
# plot random assignment
axes[0].scatter(X_scaled[:, 0], X_scaled[:, 1], c=random_clusters, cmap=mglearn.cm3, s=60)
axes[0].set_title("Random assignment: {:.2f}".format(
silhouette_score(X_scaled, random_clusters)))
algorithms = [KMeans(n_clusters=2), AgglomerativeClustering(n_clusters=2), DBSCAN()]
for ax, algorithm in zip(axes[1:], algorithms):
    clusters = algorithm.fit_predict(X_scaled)
# plot the cluster assignments and cluster centers
ax.scatter(X_scaled[:, 0], X_scaled[:, 1], c=clusters, cmap=mglearn.cm3, s=60)
ax.set_title("{} : {:.2f}".format(algorithm.__class__.__name__,
silhouette_score(X_scaled, clusters)))"""
