"""
BDA502 Machine Learning WEEK-4
Lecture Notes on Regression in Machine Learning

#Linear Regression: Adv:anlaşılır,imlementasyonu kolay -no parameter,hızlı
#Disadv:
    # outlier,
    # feature lar really independent olmayabilir-correlation-(collinearity)- 2 feature arasında çok yüksek correlation olabilir;
    bias ağırlığı artar overfit yapma ihtimali artar birinden birini koymalıyız.
    # her durumda linear ilişki olmayabilir.
    # çeşitli değerlerin baskın gelmesi (çok büyük değerler ile çok küçük değerlerin ayn ıdata da olması. Datanın dağılımı.)

#Multiple Regression:
  #underfitting: ilişkiyi açıklayamamak (çizgi saçma sapan noktaları alıyor çok fazla dışarıda kalan nokta olur)
  #overfitting: modelin ezberlemesi. Train ettik başarılı; test datası ile denedik başarısız oldu. It means overfitted!
   #collinearity: değişken çıkarma-yüksek corr olanlardan birini seç-,PCA(bu ikisinde sonucu açıklayamayabiliriz),
    #Regularization:
        1. Ridge Regression     #ceza puanı ile değişkenlerin etki değerini değiştireibiyorsunuz.
        2. Lasso Regression     #100 değişkenden şu 8 i kalsın diyoruz onlar kalıyoor.
        Genelde Ridge birinci,Ridge olmazsa Lasso. Bir de ikisinin arasında kalan Elastic Net var.
    #Soru******:Linear Regression Hangi noktada ML haline gelir? değişken seçimi, parametrelerin belirtilmesi, Data büyüklüğü-Feature fazlalığ mühendislik gerektiriyor.
"""

########################################################################################
### SIMPLE LINEAR REGRESSION ###
########################################################################################

from sklearn import linear_model
# class sklearn.linear_model.LinearRegression(fit_intercept=True, normalize=False, copy_X=True, n_jobs=1)
from sklearn.metrics import mean_squared_error, r2_score

reg = linear_model.LinearRegression()
reg = reg.fit([[0, 0], [1, 1], [2, 2]], [0, 1, 2])
print(reg.coef_)

# reg_y_pred = np.array(reg.predict([1 1]))
# print("Mean squared error: %.2f" % mean_squared_error([1, 1], 1))

# TASK-1: What do the values in the printed array show? What does it mean?

"""
This example uses the only the first feature of the diabetes dataset, in order to illustrate a two-dimensional plot of
this regression technique. The straight line can be seen in the plot, showing how linear regression attempts to draw a
straight line that will best minimize the residual sum of squares between the observed responses in the dataset, and the
responses predicted by the linear approximation. The coefficients, the residual sum of squares and the variance score
are also calculated.
"""
#Linear Regression: Adv:anlaşılır,imlementasyonu kolay -no parameter,hızlı
#Disadv:
    # outlier,
    # feature lar really independent olmayabilir-correlation-(collinearity)- 2 feature arasında çok yüksek correlation olabilir; bias ağırlığ ıartar overfit yapma ihtimali artar birinden birini koymalıyız.
    # her durumda linear ilişki olmayabilir.
    # çeşitli değerlerin baskın gelmesi (çok büyük değerler ile çok küçük değerlerin ayn ıdata da olması. Datanın dağılımı.)

print(__doc__)

import matplotlib.pyplot as plt
import numpy as np
from sklearn import datasets, linear_model
from sklearn.metrics import mean_squared_error, r2_score

# Load the diabetes dataset
diabetes = datasets.load_diabetes()

# Use only one feature
diabetes_X = diabetes.data[:, np.newaxis, 2]

# Split the data into training/testing sets
diabetes_X_train = diabetes_X[:-20]
diabetes_X_test = diabetes_X[-20:]

# Split the targets into training/testing sets
diabetes_y_train = diabetes.target[:-20]
diabetes_y_test = diabetes.target[-20:]

# Create linear regression object
regr = linear_model.LinearRegression()

# Train the model using the training sets
regr.fit(diabetes_X_train, diabetes_y_train)

# Make predictions using the testing set
diabetes_y_pred = regr.predict(diabetes_X_test)

# The coefficients
print('Coefficients: \n', regr.coef_)
# The mean squared error
print("Mean squared error: %.2f"
      % mean_squared_error(diabetes_y_test, diabetes_y_pred))
# Explained variance score: 1 is perfect prediction
print('Variance score: %.2f' % r2_score(diabetes_y_test, diabetes_y_pred))

# Plot outputs
plt.scatter(diabetes_X_test, diabetes_y_test,  color='black')
plt.plot(diabetes_X_test, diabetes_y_pred, color='blue', linewidth=3)
plt.xticks(())
plt.yticks(())
plt.show()

# TASK-2: What can you say about the success of this model that is presented both with parameters and visually?

########################################################################################
### RIDGE REGRESSION ###
########################################################################################
#
"""
Ridge regression: Linear least squares with L2 regularization.
This model solves a regression model where the loss function is the linear least squares function and regularization is
given by the l2-norm. Also known as Ridge Regression or Tikhonov regularization. This estimator has built-in support for
multi-variate regression (i.e., when y is a 2d-array of shape [n_samples, n_targets]).
Documentation: http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html
"""
#

import mglearn
import matplotlib.pyplot as plt
from sklearn.datasets import load_breast_cancer
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split  # some documents still include the cross-validation option but it no more exists in version 18.0
import pylab as plt
from sklearn.linear_model import Ridge

X, y = mglearn.datasets.load_extended_boston()
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)
lr = LinearRegression().fit(X_train, y_train)

print("Training set score: {:.2f}".format(lr.score(X_train, y_train)))
print("Test set score: {:.2f}".format(lr.score(X_test, y_test)))  # Overfit

ridge = Ridge().fit(X_train, y_train)
print("Training set score: {:.2f}".format(ridge.score(X_train, y_train)))
print("Test set score: {:.2f}".format(ridge.score(X_test, y_test)))

ridge10 = Ridge(alpha=10).fit(X_train, y_train)
print("Training set score: {:.2f}".format(ridge10.score(X_train, y_train)))
print("Test set score: {:.2f}".format(ridge10.score(X_test, y_test)))

ridge01 = Ridge(alpha=0.1).fit(X_train, y_train)
print("Training set score: {:.2f}".format(ridge01.score(X_train, y_train)))
print("Test set score: {:.2f}".format(ridge01.score(X_test, y_test)))

plt.plot(ridge.coef_, 's', label="Ridge alpha=1")
plt.plot(ridge10.coef_, '^', label="Ridge alpha=10")
plt.plot(ridge01.coef_, 'v', label="Ridge alpha=0.1")
plt.plot(lr.coef_, 'o', label="LinearRegression")

plt.xlabel("Coefficient index")
plt.ylabel("Coefficient magnitude")
xlims = plt.xlim()
plt.hlines(0, xlims[0], xlims[1])
plt.xlim(xlims)
plt.ylim(-25, 25)
plt.legend()

# TASK-3: What are different penalty scores that are attempted for this Ridge model? What is the effect of this
# penalty terms? Which one (among these three) is better and why?
#Ridge() with alpha=1;score of train data is 0.89 and it is 0.75 for test data. So, they are the closest pair for this dataset when we compare the other alpha vlaues and linear regression.
#  To sum up; we can say the second model is the best for this dataset.
#Grafiğe baktığımızda her bir  x(feature) için ne katsayısı verdiğini(coefficient) gösteriyor; yani katsayı dağılımını.
# sarı olanlar -ridge10- düz çizgiye daha yakın bu da kötü. Bazı değişkenlerin ağırlığı daha düşük olmalı bazılarnının yüksek. grafikte ridge() -alpha 1- daha iyi bir dağılım veriyor coefficientlar için.
# 106 tane değişken için verdiği coefficieentlar her bir model için: grafik.

################################
# ########################################################
### LASSO REGRESSION ###
########################################################################################
#Adv: Değişken sayısını azaltır.modelin complexity sini düşürür.
"""
Linear Model trained with L1 prior as regularizer (aka the Lasso)
The optimization objective for Lasso is:
(1 / (2 * n_samples)) * ||y - Xw||^2_2 + alpha * ||w||_1
Technically the Lasso model is optimizing the same objective function as the Elastic Net with l1_ratio=1.0 (no L2 penalty).
Documentation: http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html
"""
# bazı değişkenleri atar, model complexity sisini azaltır.
#Değişkenlerin data üzerindeki etkisine bakıp oldukça katı bir şekilde elimine eder. (sıfır yapar)
from sklearn.linear_model import Lasso

lasso = Lasso().fit(X_train, y_train)
print("Training set score: {:.2f}".format(lasso.score(X_train, y_train)))
print("Test set score: {:.2f}".format(lasso.score(X_test, y_test)))
print("Number of features used: {}".format(np.sum(lasso.coef_ != 0)))

# We increase the default setting of "max_iter",
# otherwise the model would warn us that we should increase max_iter.
lasso001 = Lasso(alpha=0.01, max_iter=100000).fit(X_train, y_train)
print("Training set score: {:.2f}".format(lasso001.score(X_train, y_train)))
print("Test set score: {:.2f}".format(lasso001.score(X_test, y_test)))
print("Number of features used: {}".format(np.sum(lasso001.coef_ != 0)))

lasso00001 = Lasso(alpha=0.0001, max_iter=100000).fit(X_train, y_train)
print("Training set score: {:.2f}".format(lasso00001.score(X_train, y_train)))
print("Test set score: {:.2f}".format(lasso00001.score(X_test, y_test)))
print("Number of features used: {}".format(np.sum(lasso00001.coef_ != 0)))

plt.plot(lasso.coef_, 's', label="Lasso alpha=1")
plt.plot(lasso001.coef_, '^', label="Lasso alpha=0.01")
plt.plot(lasso00001.coef_, 'v', label="Lasso alpha=0.0001")

plt.plot(ridge01.coef_, 'o', label="Ridge alpha=0.1")
plt.legend(ncol=2, loc=(0, 1.05))
plt.ylim(-25, 25)
plt.xlabel("Coefficient index")
plt.ylabel("Coefficient magnitude")

# TASK-4: Would you please explain these 3 different Lasso models?

########################################################################################
### LOGISTIC REGRESSION ###
########################################################################################
# http://scikit-learn.org/stable/auto_examples/linear_model/plot_logistic_l1_l2_sparsity.html#sphx-glr-auto-examples-linear-model-plot-logistic-l1-l2-sparsity-py
# Satın alır/almaz,cinsiyete bağlı kültüre bağlı değişkenleri eleek için kullanılabilir. Kategorik verileri elimine eder;
#logaritmik bir fonk geçirip normal bi dağılıma geçirirsek dağılımı; iyi bir model çıkarabilir.

from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split  # some documents still include the cross-validation option but it no more exists in version 18.0
from sklearn.linear_model import LogisticRegression
import matplotlib.pyplot as plt

cancer = load_breast_cancer()
X_train, X_test, y_train, y_test = train_test_split(
    cancer.data, cancer.target, stratify=cancer.target, random_state=42)
logreg = LogisticRegression().fit(X_train, y_train)
print("Training set score: {:.3f}".format(logreg.score(X_train, y_train)))
print("Test set score: {:.3f}".format(logreg.score(X_test, y_test)))

logreg100 = LogisticRegression(C=100).fit(X_train, y_train)
print("Training set score: {:.3f}".format(logreg100.score(X_train, y_train)))
print("Test set score: {:.3f}".format(logreg100.score(X_test, y_test)))

logreg001 = LogisticRegression(C=0.01).fit(X_train, y_train)
print("Training set score: {:.3f}".format(logreg001.score(X_train, y_train)))
print("Test set score: {:.3f}".format(logreg001.score(X_test, y_test)))

plt.plot(logreg.coef_.T, 'o', label="C=1")
plt.plot(logreg100.coef_.T, '^', label="C=100")
plt.plot(logreg001.coef_.T, 'v', label="C=0.001")
plt.xticks(range(cancer.data.shape[1]), cancer.feature_names, rotation=90)
xlims = plt.xlim()
plt.hlines(0, xlims[0], xlims[1])
plt.xlim(xlims)
plt.ylim(-5, 5)
plt.xlabel("Feature")
plt.ylabel("Coefficient magnitude")
plt.legend()



"""print(__doc__)
import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LogisticRegression
from sklearn import datasets
from sklearn.preprocessing import StandardScaler

digits = datasets.load_digits()
X, y = digits.data, digits.target
X = StandardScaler().fit_transform(X)

# classify small against large digits
y = (y > 4).astype(np.int)

# Set regularization parameter
for i, C in enumerate((100, 1, 0.01)):
    # turn down tolerance for short training time
    clf_l1_LR = LogisticRegression(C=C, penalty='l1', tol=0.01)
    clf_l2_LR = LogisticRegression(C=C, penalty='l2', tol=0.01)
    clf_l1_LR.fit(X, y)
    clf_l2_LR.fit(X, y)

    coef_l1_LR = clf_l1_LR.coef_.ravel()
    coef_l2_LR = clf_l2_LR.coef_.ravel()

    # coef_l1_LR contains zeros due to the
    # L1 sparsity inducing norm

    sparsity_l1_LR = np.mean(coef_l1_LR == 0) * 100
    sparsity_l2_LR = np.mean(coef_l2_LR == 0) * 100

    print("C=%.2f" % C)
    print("Sparsity with L1 penalty: %.2f%%" % sparsity_l1_LR)
    print("score with L1 penalty: %.4f" % clf_l1_LR.score(X, y))
    print("Sparsity with L2 penalty: %.2f%%" % sparsity_l2_LR)
    print("score with L2 penalty: %.4f" % clf_l2_LR.score(X, y))

    l1_plot = plt.subplot(3, 2, 2 * i + 1)
    l2_plot = plt.subplot(3, 2, 2 * (i + 1))
    if i == 0:
        l1_plot.set_title("L1 penalty")
        l2_plot.set_title("L2 penalty")

    l1_plot.imshow(np.abs(coef_l1_LR.reshape(8, 8)), interpolation='nearest',
                   cmap='binary', vmax=1, vmin=0)
    l2_plot.imshow(np.abs(coef_l2_LR.reshape(8, 8)), interpolation='nearest',
                   cmap='binary', vmax=1, vmin=0)
    plt.text(-8, 3, "C = %.2f" % C)

    l1_plot.set_xticks(())
    l1_plot.set_yticks(())
    l2_plot.set_xticks(())
    l2_plot.set_yticks(())

plt.show()"""

########################################################################################
### ELASTICNET REGRESSION ###
########################################################################################
#

from sklearn.linear_model import ElasticNet
from sklearn.datasets import make_regression

X, y = make_regression(n_features=2, random_state=0)
regr = ElasticNet(random_state=0)
regr.fit(X, y)
ElasticNet(alpha=1.0, copy_X=True, fit_intercept=True, l1_ratio=0.5,
      max_iter=1000, normalize=False, positive=False, precompute=False,
      random_state=0, selection='cyclic', tol=0.0001, warm_start=False)
print(regr.coef_)
print(regr.intercept_)
print(regr.predict([[0, 0]]))



########################################################################################
### LOGISTIC REGRESSION ###
########################################################################################
#

from sklearn.linear_model import LogisticRegression
from sklearn.svm import LinearSVC

X, y = mglearn.datasets.make_forge()

fig, axes = plt.subplots(1, 2, figsize=(10, 3))

for model, ax in zip([LinearSVC(), LogisticRegression()], axes):
    clf = model.fit(X, y)
    mglearn.plots.plot_2d_separator(clf, X, fill=False, eps=0.5,
                                    ax=ax, alpha=.7)
    mglearn.discrete_scatter(X[:, 0], X[:, 1], y, ax=ax)
    ax.set_title("{}".format(clf.__class__.__name__))
    ax.set_xlabel("Feature 0")
    ax.set_ylabel("Feature 1")
axes[0].legend()

mglearn.plots.plot_linear_svc_regularization()



# TASK-5:

# 1. Place the relevant file DATA.csv into the related project folder
# download it from http://www-bcf.usc.edu/~gareth/ISL/Advertising.csv
# 2. Quick Analysis and Visualization
# 3. Train linear regression model and two of other models
# 4. Show the Learned estimators/coefficients
# 5. Model Evaluation of these models and compare your findings

import numpy as np
import pandas as pd
# import model
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
from sklearn.linear_model import Ridge

# import module to calculate model perfomance metrics
from sklearn import metrics


data_path = "DATA.csv" # or load the dataset directly from the link
# # data_link = "http://www-bcf.usc.edu/~gareth/ISL/Advertising.csv"

data = pd.read_csv(data_path, index_col=0)

# create a Python list of feature names
feature_names = ['TV', 'radio','newspaper']

# use the list to select a subset of the original DataFrame
X = data[feature_names]

# getting sales as the target value
y = data.sales

# Splitting X and y into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)

# Linear Regression Model
linreg = LinearRegression()

# fit the model to the training data (learn the coefficients)
linreg.fit(X_train, y_train)

# make predictions on the testing set
y_pred = linreg.predict(X_test)

# compute the RMSE of our predictions
#print(np.sqrt(metrics.mean_squared_error(y_test, y_pred)))
print(("Mean Squared Error for Linear Regression:",metrics.mean_squared_error(y_test, y_pred)))

ridge = Ridge().fit(X_train, y_train)
y_pred=ridge.predict(X_test)
print("Mean Squared Error for Ridge():",mean_squared_error(y_test,y_pred))


ridge10 = Ridge(alpha=10).fit(X_train, y_train)
y_pred=ridge10.predict(X_test)
print("Mean Squared Error for Ridge(alpha=10):",mean_squared_error(y_test,y_pred))

ridge01 = Ridge(alpha=0.1).fit(X_train, y_train)
y_pred=ridge01.predict(X_test)
print("Mean Squared Error for Ridge(alpha=0.1):",mean_squared_error(y_test,y_pred))

#predict dediğinde y value yu karşılaştırırsın. score dediğinde R-kare leri karşılaştırırsın.
